{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82fec67-5411-4c42-a594-1e98b0a488e7",
   "metadata": {},
   "source": [
    "# Text Analysis for Digital Humanities: Fine-Tuning GPT\n",
    "\n",
    "We can compare GPT2 models that are finetuned according to different approaches!\n",
    "\n",
    "## What is fine-tuning?\n",
    "Fine-tuning GPT-2 on a specific dataset, like a collection of Irish drama texts, customizes the model's responses to reflect the themes, style, language, idioms, and character types found within that corpus. This process tailors the model's generative capabilities, making it more likely to produce outputs that are stylistically and thematically aligned with the fine-tuning material.\n",
    "\n",
    "The model's adaptation will be more pronounced when generating text related to or prompted by the domain we train it on. This might include specific narrative styles, dialogue structures, and dramatic conventions unique to the genre and cultural context.\n",
    "\n",
    "## Weight Adjustment\n",
    "Fine-tuning adjusts the weights of the neural network to minimize the loss on the new data. The changes in weights help the model better predict or generate sequences that resemble the fine-tuning data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6478974-310b-49b4-bb6e-7cace9efa711",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "\n",
    "If you want to run this code yourself, I **highly** recommend doing so in a new environment. Read more [here](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) if you want to know how this works. The PyTorch and Transformers packages require very specific versions that might clash with other installed packages.\n",
    "\n",
    "After importing, I check which versions of the packages I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642a7038-81fc-4ed4-9792-c0365f88eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc72ec-a07e-4a86-a3d7-649fb706d0c5",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "Time to start the finetuning process. We will be using GPT2, the base model with 117M parameters.\n",
    "\n",
    "We load in our dataset of Irish drama, and initialize a tokenizer.\n",
    "\n",
    "The tokenizer performs several critical tasks to convert raw text into a format that the GPT-2 model can understand:\n",
    "\n",
    "- Splitting Text into Tokens: The tokenizer breaks down input text into tokens. For GPT-2, these tokens are usually subwords or characters, allowing the model to handle a wide range of words and vocabularies efficiently.\n",
    "- Converting Tokens to IDs: Each token is mapped to a unique integer ID based on the GPT-2 vocabulary. This conversion is necessary because neural networks operate on numerical data, not raw text.\n",
    "- Adding Special Tokens: GPT-2 requires certain special tokens for its operation (e.g., end-of-text token). The tokenizer takes care of adding these tokens where appropriate.\n",
    "- Padding & Truncation: To process batches of data efficiently, all input sequences must be of the same length. The tokenizer can pad shorter sequences with a special padding token or truncate longer ones to a maximum length.\n",
    "- Creating Attention Masks: The tokenizer generates attention masks to differentiate real tokens from padding tokens. This helps the model pay attention to relevant tokens and ignore padded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6891802e-b0f1-4050-ab95-cafcd43c18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load texts from files\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "folder_path = 'data/data_cleaned'\n",
    "texts = load_texts_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaa3aaa-1268-4246-b5bd-dd040c7f13d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44653"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca2cc-ee7c-4c26-a888-0a292434ec40",
   "metadata": {},
   "source": [
    "## Approach 1: Using Truncation\n",
    "\n",
    "Let's finetune a GPT model. First, we need to tokenize our texts using GPT2's **tokenizer**. This tokenizer converts raw text input into a sequence of numerical tokens, with special tokens added for padding, special characters, and end-of-text markers, facilitating processing by the model.\n",
    "\n",
    "For our first approach, we will tokenize the entire texts with truncation and padding to a fixed maximum length. This method is straightforward and treats each text as an individual sequence for the model to learn from. The main characteristics include:\n",
    "\n",
    "- `truncation`: Texts longer than `max_length=512` are cut off, potentially losing important information at the end.\n",
    "- `padding`: Texts shorter than `max_length=512` are padded to ensure uniform sequence length, usually with the pad_token. This is not relevant to us as all texts we are feeding into the model are much longer than 512 tokens.\n",
    "\n",
    "During fine-tuning, the prediction task involves predicting the next token in the sequence based on the preceding tokens. So, for each input sequence (i.e., for each Irish drama text) consisting of its first 512 tokens, the model  predicts the next token for each token position within that sequence. The model predicts tokens from the second to the 512th position within each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644bfca0-7def-4af7-9734-5d3e46048413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec4274-17b5-4a3a-89ab-ab7243820c53",
   "metadata": {},
   "source": [
    "In the following cell, we initialize the fine-tuning process using Hugging Face's `Trainer` class.\n",
    "\n",
    "The first parameter, `model`, is the pre-trained GPT-2 model that we intend to fine-tune. It has been previously loaded and is now set to be further trained on our specific dataset to adjust its weights based on the new data, enhancing its ability to generate or understand text similar to your training corpus.\n",
    "\n",
    "`TrainingArguments` further specifies various configuration settings for the training process:\n",
    "- `output_dir`: The directory where the training outputs (like the fine-tuned model checkpoints) will be saved.\n",
    "- `num_train_epochs`: The number of times the training process should iterate over the entire dataset. Here, it's set to 3, meaning the model will see the dataset three times.\n",
    "- `per_device_train_batch_size`: The number of training examples processed per device (e.g., GPU) per training step. A batch size of 4 is specified, balancing the computational load and memory usage.\n",
    "- `logging_dir`: Directory where training logs will be saved, enabling monitoring of the training process through metrics like loss over time.\n",
    "\n",
    "Finally, `trainer.train()` starts the training process based on the specified model, training arguments, and dataset. The Trainer handles various training aspects, including feeding the input data to the model, performing backpropagation to adjust the model's weights, saving checkpoints, and logging training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32e74a3-1e2e-45f2-95e4-391f4034ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/var/folders/xp/m387b0r56b9gyh5kbcztxmvc0000gn/T/ipykernel_1295/1954384970.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 04:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=3.4367586771647134, metrics={'train_runtime': 252.5598, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.475, 'total_flos': 123852423168000.0, 'train_loss': 3.4367586771647134, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8368f6-c45c-4f6a-805b-b010a8c7e8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/finetuned_tokenizer/tokenizer_config.json',\n",
       " 'models/finetuned_tokenizer/special_tokens_map.json',\n",
       " 'models/finetuned_tokenizer/vocab.json',\n",
       " 'models/finetuned_tokenizer/merges.txt',\n",
       " 'models/finetuned_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = 'models/finetuned_model_1'\n",
    "tokenizer_save_path = 'models/finetuned_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d772f-42ee-47d1-a549-30c1bf0602ba",
   "metadata": {},
   "source": [
    "## Approach 2: Sliding Windows\n",
    "\n",
    "We have a model. The only problem is that we trained the model on a pretty limited amount of text from our corpus.\n",
    "\n",
    "Recall that we set the `max_length=512` parameter. If each token represents roughly 4 characters on average (a rough estimation since tokens can vary from parts of a word to several words depending on the tokenizer's vocabulary and the nature of the text), then 512 tokens might cover around 2048 characters. This comes down to the first lines of the first scene for each work.\n",
    "\n",
    "To make use of more of our data during finetuning, we will now implement a \"sliding windows approach\". This involves segmenting our Irish drama texts into smaller, overlapping portions (windows).\n",
    "\n",
    "In this approach, the model first processes the tokens in \"chunks\". First, the model processes the first 512 tokens, like we did before. However, after processing the first chunk, the window is moved forward by the step_size (in this case, 200 tokens). This ensures some overlap between adjacent chunks, allowing the model to capture context from nearby tokens. This process is repeated for each subsequent chunk until the entire text is covered. Each chunk is treated as an independent sequence, and the model predicts tokens within that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41bca0a-5a2d-40a1-b87e-dc1c75c67a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512  # Max tokens per chunk\n",
    "step_size = 200  # Tokens to move the window each time\n",
    "\n",
    "def create_sliding_windows(tokenizer, text, window_size, step_size):\n",
    "    # First, split the text into words or smaller units\n",
    "    words = text.split()\n",
    "    max_tokens_for_window = window_size - tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "    # Initialize\n",
    "    windows = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while start_index < len(words):\n",
    "        # Dynamically determine the end index by tokenizing a slice of words and checking the length\n",
    "        end_index = start_index + 1\n",
    "        while end_index <= len(words):\n",
    "            tokens = tokenizer.encode(' '.join(words[start_index:end_index]), add_special_tokens=True)\n",
    "            if len(tokens) > max_tokens_for_window:\n",
    "                break\n",
    "            end_index += 1\n",
    "        \n",
    "        # Adjust end_index to fit within limits, then encode\n",
    "        end_index -= 1\n",
    "        window_tokens = tokenizer.encode(' '.join(words[start_index:end_index]), add_special_tokens=True)\n",
    "        windows.append(window_tokens)\n",
    "        \n",
    "        start_index += step_size\n",
    "\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71261ec-7ad6-457f-ba2a-69954a3ad2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_windows = []  # Initialize an empty list to hold all window segments\n",
    "\n",
    "for text in texts:\n",
    "    windows = create_sliding_windows(tokenizer, text, window_size, step_size)\n",
    "    all_windows.extend(windows)  # Add the segments from this text to the collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48160a-88fe-407b-ac61-29f8c8222a51",
   "metadata": {},
   "source": [
    "This resulting `all_windows` object is a list where each element is another list. Each inner list contains the sequence of token IDs representing a segment (window) of the original text after tokenization.\n",
    "\n",
    "The below `SlidingWindowDataset` class ensures that the data fed into the model during training follows the required format. It handles tasks such as padding sequences to a consistent length, organizing the data into windows appropriate for the sliding windows approach, and preparing both input and label data for each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691caddd-07d3-42d4-b5a6-f554b686d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, token_windows):\n",
    "        # Expecting token_windows to be a list of lists (token IDs for each window)\n",
    "        self.token_windows = token_windows\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of windows\n",
    "        return len(self.token_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Accessing the token IDs for the window at the given index\n",
    "        window_token_ids = self.token_windows[idx]\n",
    "\n",
    "        # Padding: Ensure each sequence is of the same length\n",
    "        padded_window_token_ids = window_token_ids + [tokenizer.pad_token_id] * (window_size - len(window_token_ids))\n",
    "\n",
    "        # Converting the list of token IDs into a PyTorch tensor\n",
    "        input_ids = torch.tensor(padded_window_token_ids, dtype=torch.long)\n",
    "\n",
    "        # For language modeling, the labels are the same as input_ids, also padded\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Return a dictionary with input_ids and labels\n",
    "        return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "\n",
    "# Correctly creating the dataset instance with the list of windows\n",
    "train_dataset = SlidingWindowDataset(all_windows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6de59-2f30-4702-873d-172f30afce2b",
   "metadata": {},
   "source": [
    "Now, we initiate training for our second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd54ffd-bfe9-4578-a83b-021dce697c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7677' max='7677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7677/7677 5:18:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.107900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7677, training_loss=3.2442199540073027, metrics={'train_runtime': 19106.5249, 'train_samples_per_second': 1.607, 'train_steps_per_second': 0.402, 'total_flos': 8022187966464000.0, 'train_loss': 3.2442199540073027, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        logging_dir=\"./logs\",\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429730c2-d2ff-4c6e-93f5-5cec4e554bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/finetuned_tokenizer/tokenizer_config.json',\n",
       " 'models/finetuned_tokenizer/special_tokens_map.json',\n",
       " 'models/finetuned_tokenizer/vocab.json',\n",
       " 'models/finetuned_tokenizer/merges.txt',\n",
       " 'models/finetuned_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = 'models/finetuned_model_2'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec19c-14d1-40c2-93e8-2ed8dd1c1532",
   "metadata": {},
   "source": [
    "## Evaluating Outputs: Perplexity\n",
    "\n",
    "One way to evaluate the model is by calculating a **perplexity** score: a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the text. Lower perplexity indicates better performance.\n",
    "\n",
    "Perplexity is usually calculated based on the so-called \"cross-entropy loss\" of the model when predicting the next token in a sequence.\n",
    "\n",
    "Basically, we feed the model with a few texts it hasn't seen yet. The model takes a randomly selected 512 tokens from each of these texts, then does the prediction task of predicting tokens for each of these texts. These predictions are compared with the actual tokens in the input sequences to compute the loss. We check and average the loss for each of these predictions; this final score informs the perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c43b215-e5ae-4e7e-97de-fff225a32d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20446 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Perplexity: 32.5595703125\n",
      "Model 2 Perplexity: 39.908390045166016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    # Ensure the model is in evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        # Tokenize the text to find out its total length in tokens.\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        text_len = len(tokens)\n",
    "        \n",
    "        # If the text is longer than 512 tokens, choose a random start point.\n",
    "        if text_len > 512:\n",
    "            start_index = np.random.randint(0, text_len - 512)\n",
    "            end_index = start_index + 512\n",
    "            tokens = tokens[start_index:end_index]\n",
    "        else:\n",
    "            tokens = tokens[:512]  # Ensure not longer than 512 tokens\n",
    "        \n",
    "        # Decode tokens back to text.\n",
    "        modified_text = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
    "        modified_texts.append(modified_text)\n",
    "    \n",
    "    # Proceed as before but with modified_texts.\n",
    "    encodings = tokenizer(modified_texts, return_tensors='pt', padding=True, truncation=True, max_length=512, add_special_tokens=True)\n",
    "    \n",
    "    dataset = TensorDataset(encodings.input_ids, encodings.attention_mask)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_length = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch[0].to(model.device), batch[1].to(model.device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_length += input_ids.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_length\n",
    "    perplexity = torch.exp(torch.tensor(average_loss))\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage\n",
    "model_1 = GPT2LMHeadModel.from_pretrained('models/finetuned_model_1').to('cpu')\n",
    "model_2 = GPT2LMHeadModel.from_pretrained('models/finetuned_model_2').to('cpu')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "eval_texts = load_texts_from_folder('data/eval_texts')\n",
    "perplexity_1 = calculate_perplexity(model_1, tokenizer, eval_texts)\n",
    "print(f\"Model 1 Perplexity: {perplexity_1}\")\n",
    "perplexity_2 = calculate_perplexity(model_2, tokenizer, eval_texts)\n",
    "print(f\"Model 2 Perplexity: {perplexity_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f794415-eb85-4e94-9f87-fd235d304721",
   "metadata": {},
   "source": [
    "Even though `model 2` was finetuned using a lot more data, it has a lower perplexity score--meaning the model's predictions on the test data are less accurate. There could be many reasons for this; for instance, `model 2` might be overfitting to less relevant, more specific patterns in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63102fee-7a30-44db-956d-66f7fca13724",
   "metadata": {},
   "source": [
    "## Evaluating Outputs: Interpretation\n",
    "\n",
    "While perplexity offers one approach to comparing model performance, we might be more interested in the kinds of texts these models actually generate, and how informative, surprising, or inspiring they are. \n",
    "\n",
    "Let's compare the performance of the two models ourselves. We'll enter a prompt and have a look at how the two models complete it.\n",
    "\n",
    "When `do_sample=True`, the model generates text by sampling from the probability distribution of the next token given the context. This distribution is determined by the model's predictions. Instead of simply picking the most probable next token (deterministic), the model randomly selects the next token based on this probability distribution, which can introduce variety and creativity in the generated text.\n",
    "\n",
    "Parameters like `temperature`, `top_k`, and `top_p` modify this distribution to control diversity and coherence:\n",
    "\n",
    "- Temperature: Controls randomness, higher values increase diversity.\n",
    "- Top-p: The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.\n",
    "- Top-k: Sample from the k most likely next tokens at each step. Lower k focuses on higher probability tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a73eb5f3-527a-45f0-8802-a5b95181ca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text from model 1: Once upon a time, it was clear that the young King had left town and stayed in Sir Tristram's flat. But he never brought his father to see her again until after breakfast at twelve o'clock; not even on purpose for an explanation of what happened during dinner before midnight but as she continued asleep: there is no mention given where they have gone from thence except according their custom how night fell (for if God were known so all those things might be hidden); then perhaps this woman will give me further indications concerning some curious event which occurred nine days since last we saw each other--in case nothing should happen between now and evening? I cannot help dreaming more than three-quarters or six hours into our conversation about something else worthy mentioning \n",
      "\n",
      "Generated text from model 2: Once upon a time he would have found me and we were alone. A short while later (to the men at their table) Mr Marwood is about to be seized, but before that no evidence has been made of his guilt or innocence so long as it's over yet! Well then: I'm ready for him when you're finished playing our game; what with your own children--if there was not more than one child after my name? But now since they know mine well enough...what are yees like anyway...? [FINDER]: How did she die on top of this bed.--(He raises bellows.) FATHER BRESTLAIN enters through door leading away FROM HOUSE.(Goes up sideboard holding two spade\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Assuming the tokenizer is the same for both models and has been loaded previously\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load your fine-tuned models\n",
    "model_1 = GPT2LMHeadModel.from_pretrained('models/finetuned_model_1')\n",
    "model_2 = GPT2LMHeadModel.from_pretrained('models/finetuned_model_2')\n",
    "\n",
    "# Prompt to generate text from - change this!\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text_1 = generate_text(model_1, prompt, max_length=150, temperature=1)\n",
    "generated_text_2 = generate_text(model_2, prompt, max_length=150, temperature=1)\n",
    "print(\"Generated text from model 1:\", generated_text_1, '\\n')\n",
    "print(\"Generated text from model 2:\", generated_text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f5c9d-d21e-4e1e-854f-ea24bfc2ea83",
   "metadata": {},
   "source": [
    "What do you notice about the difference between the output of `model 1` and `model 2`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
