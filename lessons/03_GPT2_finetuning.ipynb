{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7cfced-157b-4977-b7d6-bf274f3008cc",
   "metadata": {},
   "source": [
    "# Text Analysis for Digital Humanities: Fine-Tuning GPT-2\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand GPT-2 as a large language model.\n",
    "* Reviewing output from pretrained GPT-2 model using the `transformers` library.\n",
    "* Understand the hyperparameters for GPT-2's output generation.\n",
    "* Learn how to finetune GPT-2 on the Irish dataset using a truncation and sliding windows approach.\n",
    "* Evaluate differently finetuned GPT-2 models through perplexity scores and manual interpretation.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "\n",
    "### Sections\n",
    "1. [GPT-2](#gpt2)\n",
    "2. [Finetuning GPT-2](#ft)\n",
    "3. [Evaluation: Perplexity](#perp)\n",
    "4. [Evaluation: Interpretation](#int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fec67-5411-4c42-a594-1e98b0a488e7",
   "metadata": {},
   "source": [
    "<a id='gpt2'></a>\n",
    "# GPT-2\n",
    "\n",
    "GPT-2 is a large language model from 2019. It was trained on a dataset of 8 million web pages, with the objective to predict the next word, given all of the previous words within some text. \n",
    "\n",
    "In this notebook, we are using GPT-2 Small, the base model, which has only (!) 117 million parameters (for comparison, GPT-2 XL has 1.5 billion parameters, and GPT4 has 175 *billion*). A \"parameter\" is an adjustable internal value that the model learns and uses to make predictions or generate text based on its input. More parameters usually means a \"smarter\", more capable model.\n",
    "\n",
    "We use a smaller model to make it possible to finetune it on a local computer (although you still need a pretty recent machine to run the code in this notebook). \n",
    "\n",
    "## What is fine-tuning?\n",
    "Fine-tuning GPT-2 on a specific dataset, like a collection of Irish drama texts, customizes the model's responses to reflect the themes, style, language, idioms, and character types found within that corpus. This process tailors the model's generative capabilities, making it more likely to produce outputs that are stylistically and thematically aligned with the fine-tuning material.\n",
    "\n",
    "The model's adaptation will be more pronounced when generating text related to or prompted by the domain we train it on. This might include specific narrative styles, dialogue structures, and dramatic conventions unique to the genre and cultural context.\n",
    "\n",
    "## Weight Adjustment\n",
    "Fine-tuning adjusts the weights of the neural network to minimize the loss on the new data. The changes in weights help the model better predict or generate sequences that resemble the fine-tuning data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6478974-310b-49b4-bb6e-7cace9efa711",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "\n",
    "‚ö†Ô∏è **Warning:** If you want to run this code yourself, I **highly** recommend doing so in a new environment. Read more [here](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) if you want to know how this works. The PyTorch and Transformers packages require very specific versions that might clash with other installed packages.\n",
    "\n",
    "After importing, check which versions of the packages we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642a7038-81fc-4ed4-9792-c0365f88eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b9a5d-a745-4ae8-a00f-823749a0ae95",
   "metadata": {},
   "source": [
    "Let's start by having a look at what kind of text this older GPT model outputs before finetuning. In the following code, we are loading the model, feeding it with an input text, and generating output. This is called **inference** (as opposed to training).\n",
    "\n",
    "On the back end, the model is tokenizes our input text into subwords (or tokens), and then maps each token onto an embedding vector (i.e., a long list of numbers), which it has learned during training. The model then uses these embeddings to predict the most-likely words to appear after our input text ends. It then draws on probability distribution to generate text, word by word.\n",
    "\n",
    "A few things to note about the arguments we pass into `model.generate`:\n",
    "\n",
    "- When `do_sample=True`, the model generates text by **sampling** from the probability distribution of the next token given the context. This distribution is determined by the model's predictions. Instead of simply picking the most probable next token (deterministic), the model randomly selects the next token based on this probability distribution, which can introduce variety and creativity in the generated text.\n",
    "- `max_length` tells the model how many tokens its response should be.\n",
    "- `repetition_penalty`\n",
    "- Parameters like `temperature`, `top_k`, and `top_p` modify this distribution to control diversity and coherence of the output:\n",
    "    - `temperature`: Controls the randomness of the generated output. Higher temperature makes the output distribution more uniform, so you are likely to get more diverse generations\n",
    "    - `top-p`: Samples tokens with the highest probability scores until the sum of the scores reaches the specified threshold value. \n",
    "    - `top-k`: Samples tokens with the highest probabilities until the specified number of tokens is reached. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00216452-6fd2-48e8-a6fc-e77a96e5fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, when we were younger and for various reasons had grown up in small towns or cities where there was no public roads [for the rest of our lives], driving cars through town seemed to be at times like being thrown from an automobile because it couldn't handle anything more than four miles per hour ‚Äî I could feel this feeling.\"\n",
      ".\n",
      "\n",
      "\"It would have been better if everyone felt alike on their own side; but that's not what has happened here,\" he says. \"There is something important going on behind closed doors ‚Ä¶ But then as people grew older (people who now work are making less money), they're actually getting closer together so you never know how things might turn out after 15 years. And yet these days all\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Encode input context to get token IDs\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, \n",
    "                        do_sample=True, \n",
    "                        max_length=150, \n",
    "                        repetition_penalty=1.1,\n",
    "                        temperature=1, \n",
    "                        top_k=50, \n",
    "                        top_p=0.95\n",
    "                        )\n",
    "\n",
    "# Decode the generated IDs to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc72ec-a07e-4a86-a3d7-649fb706d0c5",
   "metadata": {},
   "source": [
    "<a id=\"ft\"></a>\n",
    "\n",
    "# Finetuning GPT-2\n",
    "Time to start the finetuning process. We first load in our dataset of Irish drama, and initialize a tokenizer.\n",
    "\n",
    "The tokenizer performs several critical tasks to convert raw text into a format that the GPT-2 model can understand:\n",
    "\n",
    "- Splitting Text into Tokens: The tokenizer breaks down input text into tokens. For GPT-2, these tokens are usually subwords or characters, allowing the model to handle a wide range of words and vocabularies efficiently.\n",
    "- Converting Tokens to IDs: Each token is mapped to a unique integer ID based on the GPT-2 vocabulary. This conversion is necessary because neural networks operate on numerical data, not raw text.\n",
    "- Adding Special Tokens: GPT-2 requires certain special tokens for its operation (e.g., end-of-text token). The tokenizer takes care of adding these tokens where appropriate.\n",
    "- Padding & Truncation: To process batches of data efficiently, all input sequences must be of the same length. The tokenizer can pad shorter sequences with a special padding token or truncate longer ones to a maximum length.\n",
    "- Creating Attention Masks: The tokenizer generates attention masks to differentiate real tokens from padding tokens. This helps the model pay attention to relevant tokens and ignore padded areas.\n",
    "\n",
    "‚ö†Ô∏è **Warning:** The `data_cleaned` folder with data we are using to finetune does not live in this repository due to copyright reasons -- contact us if you need access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6891802e-b0f1-4050-ab95-cafcd43c18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Load texts from files\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "# this below folder is not saved in the repo -- contact us if you need access\n",
    "folder_path = '../data/data_cleaned'\n",
    "texts = load_texts_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaa3aaa-1268-4246-b5bd-dd040c7f13d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44653"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ca2cc-ee7c-4c26-a888-0a292434ec40",
   "metadata": {},
   "source": [
    "## Approach 1: Using Truncation\n",
    "\n",
    "Let's finetune a GPT model. First, we need to tokenize our texts using GPT-2's **tokenizer**. This tokenizer converts raw text input into a sequence of numerical tokens, with special tokens added for padding, special characters, and end-of-text markers, facilitating processing by the model.\n",
    "\n",
    "For our first approach, we will tokenize the entire texts with truncation and padding to a fixed maximum length. This method is straightforward and treats each text as an individual sequence for the model to learn from. The main characteristics include:\n",
    "\n",
    "- `truncation`: Texts longer than `max_length=512` are cut off, potentially losing important information at the end.\n",
    "- `padding`: Texts shorter than `max_length=512` are padded to ensure uniform sequence length, usually with the pad_token. This is not relevant to us as all texts we are feeding into the model are much longer than 512 tokens.\n",
    "\n",
    "During fine-tuning, the prediction task involves predicting the next token in the sequence based on the preceding tokens. So, for each input sequence (i.e., for each Irish drama text) consisting of its first 512 tokens, the model  predicts the next token for each token position within that sequence. The model predicts tokens from the second to the 512th position within each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644bfca0-7def-4af7-9734-5d3e46048413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY\n",
    "\n",
    "# Initialize tokenizer with padding token set\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize texts\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # For language modeling, the labels are the input_ids shifted by one\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = TextDataset(encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec4274-17b5-4a3a-89ab-ab7243820c53",
   "metadata": {},
   "source": [
    "In the following cell, we initialize the fine-tuning process using Hugging Face's `Trainer` class.\n",
    "\n",
    "The first parameter, `model`, is the pre-trained GPT-2 model that we intend to fine-tune. It has been previously loaded and is now set to be further trained on our specific dataset to adjust its weights based on the new data, enhancing its ability to generate or understand text similar to your training corpus.\n",
    "\n",
    "`TrainingArguments` further specifies various configuration settings for the training process:\n",
    "- `output_dir`: The directory where the training outputs (like the fine-tuned model checkpoints) will be saved.\n",
    "- `num_train_epochs`: The number of times the training process should iterate over the entire dataset. Here, it's set to 3, meaning the model will see the dataset three times.\n",
    "- `per_device_train_batch_size`: The number of training examples processed per device (e.g., GPU) per training step. A batch size of 4 is specified, balancing the computational load and memory usage.\n",
    "- `logging_dir`: Directory where training logs will be saved, enabling monitoring of the training process through metrics like loss over time.\n",
    "\n",
    "Finally, `trainer.train()` starts the training process based on the specified model, training arguments, and dataset. The Trainer handles various training aspects, including feeding the input data to the model, performing backpropagation to adjust the model's weights, saving checkpoints, and logging training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32e74a3-1e2e-45f2-95e4-391f4034ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/var/folders/xp/m387b0r56b9gyh5kbcztxmvc0000gn/T/ipykernel_1295/1954384970.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 04:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=3.4367586771647134, metrics={'train_runtime': 252.5598, 'train_samples_per_second': 1.877, 'train_steps_per_second': 0.475, 'total_flos': 123852423168000.0, 'train_loss': 3.4367586771647134, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='../logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8368f6-c45c-4f6a-805b-b010a8c7e8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/finetuned_tokenizer/tokenizer_config.json',\n",
       " 'models/finetuned_tokenizer/special_tokens_map.json',\n",
       " 'models/finetuned_tokenizer/vocab.json',\n",
       " 'models/finetuned_tokenizer/merges.txt',\n",
       " 'models/finetuned_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = '../models/finetuned_model_1'\n",
    "tokenizer_save_path = '../models/finetuned_tokenizer'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d772f-42ee-47d1-a549-30c1bf0602ba",
   "metadata": {},
   "source": [
    "## Approach 2: Sliding Windows\n",
    "\n",
    "We have a model. The only problem is that we trained the model on a pretty limited amount of text from our corpus.\n",
    "\n",
    "Recall that we set the `max_length=512` parameter. If each token represents roughly 4 characters on average (a rough estimation since tokens can vary from parts of a word to several words depending on the tokenizer's vocabulary and the nature of the text), then 512 tokens might cover around 2048 characters. This comes down to the first lines of the first scene for each work.\n",
    "\n",
    "To make use of more of our data during finetuning, we will now implement a \"sliding windows approach\". This involves segmenting our Irish drama texts into smaller, overlapping portions (windows).\n",
    "\n",
    "In this approach, the model first processes the tokens in \"chunks\". First, the model processes the first 512 tokens, like we did before. However, after processing the first chunk, the window is moved forward by the step_size (in this case, 200 tokens). This ensures some overlap between adjacent chunks, allowing the model to capture context from nearby tokens. This process is repeated for each subsequent chunk until the entire text is covered. Each chunk is treated as an independent sequence, and the model predicts tokens within that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d41bca0a-5a2d-40a1-b87e-dc1c75c67a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 512  # Max tokens per chunk\n",
    "step_size = 200  # Tokens to move the window each time\n",
    "\n",
    "def create_sliding_windows(tokenizer, text, window_size, step_size):\n",
    "    # First, split the text into words or smaller units\n",
    "    words = text.split()\n",
    "    max_tokens_for_window = window_size - tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "    # Initialize\n",
    "    windows = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while start_index < len(words):\n",
    "        # Dynamically determine the end index by tokenizing a slice of words and checking the length\n",
    "        end_index = start_index + 1\n",
    "        while end_index <= len(words):\n",
    "            tokens = tokenizer.encode(' '.join(words[start_index:end_index]), add_special_tokens=True)\n",
    "            if len(tokens) > max_tokens_for_window:\n",
    "                break\n",
    "            end_index += 1\n",
    "        \n",
    "        # Adjust end_index to fit within limits, then encode\n",
    "        end_index -= 1\n",
    "        window_tokens = tokenizer.encode(' '.join(words[start_index:end_index]), add_special_tokens=True)\n",
    "        windows.append(window_tokens)\n",
    "        \n",
    "        start_index += step_size\n",
    "\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71261ec-7ad6-457f-ba2a-69954a3ad2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_windows = []  # Initialize an empty list to hold all window segments\n",
    "\n",
    "for text in texts:\n",
    "    windows = create_sliding_windows(tokenizer, text, window_size, step_size)\n",
    "    all_windows.extend(windows)  # Add the segments from this text to the collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48160a-88fe-407b-ac61-29f8c8222a51",
   "metadata": {},
   "source": [
    "This resulting `all_windows` object is a list where each element is another list. Each inner list contains the sequence of token IDs representing a segment (window) of the original text after tokenization.\n",
    "\n",
    "The below `SlidingWindowDataset` class ensures that the data fed into the model during training follows the required format. It handles tasks such as padding sequences to a consistent length, organizing the data into windows appropriate for the sliding windows approach, and preparing both input and label data for each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691caddd-07d3-42d4-b5a6-f554b686d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, token_windows):\n",
    "        # Expecting token_windows to be a list of lists (token IDs for each window)\n",
    "        self.token_windows = token_windows\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of windows\n",
    "        return len(self.token_windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Accessing the token IDs for the window at the given index\n",
    "        window_token_ids = self.token_windows[idx]\n",
    "\n",
    "        # Padding: Ensure each sequence is of the same length\n",
    "        padded_window_token_ids = window_token_ids + [tokenizer.pad_token_id] * (window_size - len(window_token_ids))\n",
    "\n",
    "        # Converting the list of token IDs into a PyTorch tensor\n",
    "        input_ids = torch.tensor(padded_window_token_ids, dtype=torch.long)\n",
    "\n",
    "        # For language modeling, the labels are the same as input_ids, also padded\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Return a dictionary with input_ids and labels\n",
    "        return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "\n",
    "# Correctly creating the dataset instance with the list of windows\n",
    "train_dataset = SlidingWindowDataset(all_windows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6de59-2f30-4702-873d-172f30afce2b",
   "metadata": {},
   "source": [
    "Now, we initiate training for our second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd54ffd-bfe9-4578-a83b-021dce697c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7677' max='7677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7677/7677 5:18:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.125200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.107900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7677, training_loss=3.2442199540073027, metrics={'train_runtime': 19106.5249, 'train_samples_per_second': 1.607, 'train_steps_per_second': 0.402, 'total_flos': 8022187966464000.0, 'train_loss': 3.2442199540073027, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"../results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        logging_dir=\"../logs\",\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429730c2-d2ff-4c6e-93f5-5cec4e554bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/finetuned_tokenizer/tokenizer_config.json',\n",
       " 'models/finetuned_tokenizer/special_tokens_map.json',\n",
       " 'models/finetuned_tokenizer/vocab.json',\n",
       " 'models/finetuned_tokenizer/merges.txt',\n",
       " 'models/finetuned_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = '../models/finetuned_model_2'\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec19c-14d1-40c2-93e8-2ed8dd1c1532",
   "metadata": {},
   "source": [
    "<a id=\"perp\"></a>\n",
    "# Evaluation: Perplexity\n",
    "\n",
    "One way to evaluate the model is by calculating a **perplexity** score: a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the text. Lower perplexity indicates better performance.\n",
    "\n",
    "Perplexity is usually calculated based on the so-called \"cross-entropy loss\" of the model when predicting the next token in a sequence.\n",
    "\n",
    "Basically, we feed the model with a few texts it hasn't seen yet. The model takes a randomly selected 512 tokens from each of these texts, then does the prediction task of predicting tokens for each of these texts. These predictions are compared with the actual tokens in the input sequences to compute the loss. We check and average the loss for each of these predictions; this final score informs the perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c43b215-e5ae-4e7e-97de-fff225a32d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20446 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Perplexity: 32.5595703125\n",
      "Model 2 Perplexity: 39.908390045166016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, batch_size=4):\n",
    "    # Ensure the model is in evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    modified_texts = []\n",
    "    for text in texts:\n",
    "        # Tokenize the text to find out its total length in tokens.\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        text_len = len(tokens)\n",
    "        \n",
    "        # If the text is longer than 512 tokens, choose a random start point.\n",
    "        if text_len > 512:\n",
    "            start_index = np.random.randint(0, text_len - 512)\n",
    "            end_index = start_index + 512\n",
    "            tokens = tokens[start_index:end_index]\n",
    "        else:\n",
    "            tokens = tokens[:512]  # Ensure not longer than 512 tokens\n",
    "        \n",
    "        # Decode tokens back to text.\n",
    "        modified_text = tokenizer.decode(tokens, clean_up_tokenization_spaces=True)\n",
    "        modified_texts.append(modified_text)\n",
    "    \n",
    "    # Proceed as before but with modified_texts.\n",
    "    encodings = tokenizer(modified_texts, return_tensors='pt', padding=True, truncation=True, max_length=512, add_special_tokens=True)\n",
    "    \n",
    "    dataset = TensorDataset(encodings.input_ids, encodings.attention_mask)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_length = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch[0].to(model.device), batch[1].to(model.device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_length += input_ids.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_length\n",
    "    perplexity = torch.exp(torch.tensor(average_loss))\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage\n",
    "model_1 = GPT2LMHeadModel.from_pretrained('../models/finetuned_model_1').to('cpu')\n",
    "model_2 = GPT2LMHeadModel.from_pretrained('../models/finetuned_model_2').to('cpu')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "eval_texts = load_texts_from_folder('../data/eval_texts')\n",
    "perplexity_1 = calculate_perplexity(model_1, tokenizer, eval_texts)\n",
    "print(f\"Model 1 Perplexity: {perplexity_1}\")\n",
    "perplexity_2 = calculate_perplexity(model_2, tokenizer, eval_texts)\n",
    "print(f\"Model 2 Perplexity: {perplexity_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f794415-eb85-4e94-9f87-fd235d304721",
   "metadata": {},
   "source": [
    "Even though `model 2` was finetuned using a lot more data, it has a lower perplexity score--meaning the model's predictions on the test data are less accurate. There could be many reasons for this; for instance, `model 2` might be overfitting to less relevant, more specific patterns in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63102fee-7a30-44db-956d-66f7fca13724",
   "metadata": {},
   "source": [
    "<a id=\"int\"></a>\n",
    "# Evaluation: Interpretation\n",
    "\n",
    "While perplexity offers one approach to comparing model performance, we might be more interested in the kinds of texts these models actually generate, and how informative, surprising, or inspiring they are. \n",
    "\n",
    "Let's compare the performance of the two models ourselves. We'll enter a prompt and have a look at how the two models complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73eb5f3-527a-45f0-8802-a5b95181ca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text from model 1: Once upon a time, in the summer of 1690s, he was engaged at an Indian village. This family were near Sionia and there they had many children: young women with smallpox (pupils). Mr. Pancha heard no news except from Mrs; she said that her husband died on Saturday morning while driving home to his own house‚Äîall because it went over land next road into south-east Wales for another week or so before going out again once more [the day after.] That brought him back down,\" continues Ms., \"that little girl is named Tumayee Chant√©.\"\n",
      "For some reason this name fits well among those who have been mentioned by Dr Brown as having occurred early enough along their \n",
      "\n",
      "Generated text from model 2: Once upon a time they had been known as soldiers, but for the sake of their glory--by me in my dream---- MALACHI (turning back suddenly) But this was not good news. They said you were coming and going away? BRODAR: That I should have come to that place no more than thirty minutes ago; it would be bad news if he never returned home from prison again.... There is nothing else between us now except what has happened thus far... [MORRIS enters slowly at once by door.] LADY TEMPLE: Ah!--(She goes out with some papers.) We cannot help any one talking about him-his wife‚Äôs dead or his mother's sick without putting up\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Set the seed for PyTorch (controls randomness for reproducibility)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Assuming the tokenizer is the same for both models and has been loaded previously\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def generate_text(model, prompt, do_sample=True, max_length=50, temperature=1, top_k=50, top_p=0.95, repetition_penalty=1.1):\n",
    "    \"\"\"\n",
    "    Generates text based on a given prompt using the specified model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The fine-tuned model to use for text generation.\n",
    "    - prompt: The initial text to start generating from.\n",
    "    - max_length: Maximum length of the generated text.\n",
    "    - temperature: Sampling temperature for generating text.\n",
    "    - top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.\n",
    "    - top_p: Nucleus sampling's cumulative probability cutoff to keep for top-p filtering.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_text: The generated text as a string.\n",
    "    \"\"\"\n",
    "    # Encode the prompt text to tensor\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate a sequence of tokens following the prompt\n",
    "    output_ids = model.generate(input_ids, max_length=max_length, \n",
    "                                temperature=temperature, \n",
    "                                do_sample=do_sample, \n",
    "                                top_k=top_k, \n",
    "                                top_p=top_p, \n",
    "                                repetition_penalty=repetition_penalty, \n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load your fine-tuned models\n",
    "model_1 = GPT2LMHeadModel.from_pretrained('../models/finetuned_model_1')\n",
    "model_2 = GPT2LMHeadModel.from_pretrained('../models/finetuned_model_2')\n",
    "\n",
    "# Prompt to generate text from - change this!\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Generate texts\n",
    "generated_text_1 = generate_text(model_1, prompt, max_length=150, temperature=1)\n",
    "generated_text_2 = generate_text(model_2, prompt, max_length=150, temperature=1)\n",
    "print(\"Generated text from model 1:\", generated_text_1, '\\n')\n",
    "print(\"Generated text from model 2:\", generated_text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f5c9d-d21e-4e1e-854f-ea24bfc2ea83",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the difference between the output of `model 1` and `model 2`? And how do the model differ from the pretrained model we reviewed at the start of this notebook?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "dlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
